{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b223e4a7-8ac7-412a-99b7-bd996dbaa8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best parameters: {'C': 100, 'penalty': 'l2'}\n",
      "Test Accuracy: 0.9990449813771368\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8971\n",
      "           1       1.00      1.00      1.00      1500\n",
      "\n",
      "    accuracy                           1.00     10471\n",
      "   macro avg       1.00      1.00      1.00     10471\n",
      "weighted avg       1.00      1.00      1.00     10471\n",
      "\n",
      "Model and vectorizer saved successfully.\n",
      "Sample text classified as: News with confidence 0.9698\n",
      "Notebook execution complete.\n"
     ]
    }
   ],
   "source": [
    "# This notebook demonstrates building a classifier to distinguish between news and non-news articles using the sampled non-news dataset and an existing news dataset.\n",
    "# We use descriptions_sampled.txt as our non-news sample, and Fake.csv & True.csv as our news dataset.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import random\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Load the non-news data from descriptions_sampled.txt\n",
    "non_news_path = 'News_Dataset/descriptions_sampled.txt'\n",
    "\n",
    "# For the non-news data, we assume each line is a separate document.\n",
    "with open(non_news_path, 'r', encoding='utf-8') as f:\n",
    "    non_news_docs = f.readlines()\n",
    "\n",
    "non_news_docs = [doc.strip() for doc in non_news_docs if doc.strip()]\n",
    "\n",
    "# Create a DataFrame for non-news articles and label as 0\n",
    "non_news_df = pd.DataFrame({'text': non_news_docs, 'label': 0})\n",
    "\n",
    "# Step 2: Load news data from Fake.csv and True.csv and label them as 1.\n",
    "# We assume Fake.csv and True.csv are in the working directory.\n",
    "news_fake = pd.read_csv('News_Dataset/Fake.csv')\n",
    "news_true = pd.read_csv('News_Dataset/True.csv')\n",
    "\n",
    "# They are already labeled in our previous model, but for our purpose news=1\n",
    "news_fake['label'] = 1\n",
    "news_true['label'] = 1\n",
    "\n",
    "# For the news data, we will use the 'text' column. Some datasets may require preprocessing.\n",
    "news_df = pd.concat([news_fake[['text', 'label']], news_true[['text', 'label']]], ignore_index=True)\n",
    "\n",
    "# To reduce computation time, we may sample a part of the news data if very large\n",
    "if len(news_df) > 5000:\n",
    "    news_df = news_df.sample(n=5000, random_state=42)\n",
    "\n",
    "# Combine non-news and news into one dataset\n",
    "combined_df = pd.concat([news_df, non_news_df], ignore_index=True)\n",
    "\n",
    "# Shuffle data\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Function for simple text preprocessing\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "combined_df['clean_text'] = combined_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Step 3: Split the data for training and validation\n",
    "X = combined_df['clean_text']\n",
    "y = combined_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 4: Use TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=3)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Step 5: Train a Logistic Regression classifier and optimize hyperparameters using Grid Search\n",
    "# We use a pipeline: but here we'll do grid search over C and penalty\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'], # l1 requires solver=saga; we could test both if needed\n",
    "    # Note: 'max_iter': [100, 200] can be tuned if convergence issues occur\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "accuracy_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_val)\n",
    "print(\"\\\n",
    "Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Save the model and vectorizer for later prediction\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer_news_classifier.pkl')\n",
    "joblib.dump(best_model, 'logistic_news_classifier.pkl')\n",
    "\n",
    "print(\"\\\n",
    "Model and vectorizer saved successfully.\")\n",
    "\n",
    "# Step 7: Function for predicting if an uploaded article is news or non-news\n",
    "\n",
    "def predict_news_article(text):\n",
    "    processed = preprocess_text(text)\n",
    "    vec = vectorizer.transform([processed])\n",
    "    pred = best_model.predict(vec)[0]\n",
    "    prob = best_model.predict_proba(vec).max()\n",
    "    label = 'News' if pred == 1 else 'Non-News'\n",
    "    return label, prob\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \"Breaking news: The stock market experienced a dramatic fall today due to unexpected economic reports.\"\n",
    "label, conf = predict_news_article(sample_text)\n",
    "print(f\"Sample text classified as: {label} with confidence {conf:.4f}\")\n",
    "\n",
    "print(\"Notebook execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435d4a7-d938-469b-976e-578425cf1024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
